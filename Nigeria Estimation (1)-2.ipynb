{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting CFEDemands>=0.4.1\n",
      "  Using cached CFEDemands-0.4.1-py2.py3-none-any.whl (39 kB)\n",
      "Collecting ConsumerDemands\n",
      "  Using cached ConsumerDemands-0.3.dev0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: gspread>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (4.0.1)\n",
      "Requirement already satisfied: matplotlib>=3.3.4 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.21.5 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (1.21.5)\n",
      "Collecting oauth2client>=4.1.3\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: pandas>=1.3.5 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 25)) (1.3.5)\n",
      "Requirement already satisfied: plotly>=5.1.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 28)) (5.2.1)\n",
      "Collecting eep153_tools>=0.11\n",
      "  Using cached eep153_tools-0.11-py2.py3-none-any.whl (4.4 kB)\n",
      "Processing /home/jovyan/.cache/pip/wheels/20/7e/30/7d702acd6a1e89911301cd9dbf9cb9870ca80c0e64bc2cde23/gnupg-2.3.1-py3-none-any.whl\n",
      "Requirement already satisfied: google-auth>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from gspread>=4.0.1->-r requirements.txt (line 10)) (2.6.2)\n",
      "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from gspread>=4.0.1->-r requirements.txt (line 10)) (0.4.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.3.4->-r requirements.txt (line 13)) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.3.4->-r requirements.txt (line 13)) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.3.4->-r requirements.txt (line 13)) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.3.4->-r requirements.txt (line 13)) (2.8.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.3.4->-r requirements.txt (line 13)) (8.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.9/site-packages (from oauth2client>=4.1.3->-r requirements.txt (line 20)) (0.2.8)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in /opt/conda/lib/python3.9/site-packages (from oauth2client>=4.1.3->-r requirements.txt (line 20)) (0.20.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.9/site-packages (from oauth2client>=4.1.3->-r requirements.txt (line 20)) (0.4.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from oauth2client>=4.1.3->-r requirements.txt (line 20)) (4.8)\n",
      "Requirement already satisfied: six>=1.6.1 in /opt/conda/lib/python3.9/site-packages (from oauth2client>=4.1.3->-r requirements.txt (line 20)) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.3.5->-r requirements.txt (line 25)) (2021.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from plotly>=5.1.0->-r requirements.txt (line 28)) (8.0.1)\n",
      "Requirement already satisfied: psutil>=1.2.1 in /opt/conda/lib/python3.9/site-packages (from gnupg->-r requirements.txt (line 31)) (5.9.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth>=1.12.0->gspread>=4.0.1->-r requirements.txt (line 10)) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib>=0.4.1->gspread>=4.0.1->-r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=4.0.1->-r requirements.txt (line 10)) (3.2.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=4.0.1->-r requirements.txt (line 10)) (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=4.0.1->-r requirements.txt (line 10)) (1.25.7)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /opt/conda/lib/python3.9/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=4.0.1->-r requirements.txt (line 10)) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=4.0.1->-r requirements.txt (line 10)) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /opt/conda/lib/python3.9/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread>=4.0.1->-r requirements.txt (line 10)) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Sheet to DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by defining a dictionary that contains the spreadsheet key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nigeria_data = '17L5cDhXRLNAckP3JvBLTLSYIguFqP2ebMvQLH96c0n4'\n",
    "nigeria_production = '1kG_fVBmj9EEF9LOwxN30HBxkQENOoWeQjVPYzMJe3b4-8DA'\n",
    "nigeria_consumption = '1Gzu0g2Tjp0heKYk-r5l7gVJk93k2_mHgWOtZMVkqSGI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the spreadsheet defined, grab it and define a couple of\n",
    "dataframes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from eep153_tools.sheets import read_sheets\n",
    "\n",
    "expend = read_sheets(nigeria_data,sheet='Expenditures')\n",
    "expend.columns.name = 'i'\n",
    "                 \n",
    "# Change 'ICRISAT' to key of your own sheet in Sheets, above\n",
    "hh_char = read_sheets(nigeria_data,sheet=\"HH Characteristics\")\n",
    "hh_char.columns.name = 'k'\n",
    "\n",
    "# Assume a single market: (Comment this out to make each village its own market)\n",
    "hh_char['m'] = 1\n",
    "expend['m'] = 1\n",
    "\n",
    "# x may have duplicate columns\n",
    "expend = expend.groupby('i',axis=1).sum()\n",
    "expend = expend.apply(lambda x: pd.to_numeric(x,errors='coerce'))\n",
    "expend = expend.replace(0,np.nan)\n",
    "\n",
    "# Take logs of expenditures; call this y\n",
    "log_expend = np.log(expend.set_index(['j','t','m']))\n",
    "           \n",
    "hh_char.set_index(['j','t','m'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the new Data Frame in order to group by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expend = expend.set_index(['t','j','m']).sort_index()\n",
    "expend = expend.replace(0.0,np.nan) # Replace zeroes with np.nan\n",
    "expend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People per Household, Total Expenditures, and Expenditures per Capita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = hh_char.sum(axis=1)\n",
    "num_people = pd.DataFrame(people)\n",
    "num_people = num_people.rename(columns={0:'People per HH'})\n",
    "num_people = num_people.reset_index().set_index(['t','j','m']).sort_index()\n",
    "num_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_expend = expend.iloc[:, 0:124].sum(axis=1)\n",
    "total = pd.DataFrame(total_expend)\n",
    "total = total.rename(columns={0:'Total Expenditures'})\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expend['Total Expenditures'] = total['Total Expenditures']\n",
    "expend['People per HH'] = num_people['People per HH']\n",
    "expend['Expenditures per capita'] = expend['Total Expenditures'] / expend['People per HH']\n",
    "expend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expend.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting into Quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_year(df, year):\n",
    "    new_df = df.loc[[year]]\n",
    "    return new_df\n",
    "\n",
    "def quartiles_by_te(df, year, quartile):\n",
    "    # Selecting out one year, sorting by TE, then filtering out the HHs that spent nothing\n",
    "    one_year_df = one_year(df, year)\n",
    "    one_year_df = one_year_df.reset_index().sort_values('Total Expenditures', axis=0).replace(0,np.nan)\n",
    "    one_year_df = one_year_df.dropna(axis=0, how='any', subset=['Total Expenditures'])\n",
    "    \n",
    "    # Number of rows for each quartile\n",
    "    total_rows = len(one_year_df)\n",
    "    rows_per_qtr = round(total_rows / 4)\n",
    "    \n",
    "    # Selecting the necessary rows for each quartile\n",
    "    if quartile == 1:\n",
    "        return one_year_df.iloc[0:rows_per_qtr-1]\n",
    "    else:\n",
    "        first_row = (quartile-1) * rows_per_qtr\n",
    "        last_row = (quartile * rows_per_qtr) - 1\n",
    "        return one_year_df.iloc[first_row:last_row]\n",
    "    \n",
    "def quartiles_by_epc(df, year, quartile):\n",
    "    # Selecting out one year, sorting by EPC, then filtering out the HHs that spent nothing\n",
    "    one_year_df = one_year(df, year)\n",
    "    one_year_df = one_year_df.reset_index().sort_values('Expenditures per capita', axis=0).replace(0,np.nan)\n",
    "    one_year_df = one_year_df.dropna(axis=0, how='any', subset=['Expenditures per capita'])\n",
    "    \n",
    "    # Number of rows for each quartile\n",
    "    total_rows = len(one_year_df)\n",
    "    rows_per_qtr = round(total_rows / 4)\n",
    "    \n",
    "    # Selecting the necessary rows for each quartile\n",
    "    if quartile == 1:\n",
    "        return one_year_df.iloc[0:rows_per_qtr-1]\n",
    "    else:\n",
    "        first_row = (quartile-1) * rows_per_qtr\n",
    "        last_row = (quartile * rows_per_qtr) - 1\n",
    "        return one_year_df.iloc[first_row:last_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_year(expend, 2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_qtr_by_te_2010 = quartiles_by_te(expend, 2010, 1)\n",
    "first_qtr_by_te_2010\n",
    "#te is total expenditure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_10 = quartiles_by_epc(expend, 2010, 1)\n",
    "Q1_12 = quartiles_by_epc(expend, 2012, 1)\n",
    "Q1_15 = quartiles_by_epc(expend, 2015, 1)\n",
    "Q1_18 = quartiles_by_epc(expend, 2018, 1)\n",
    "Q1 = pd.concat([Q1_10, Q1_12, Q1_15, Q1_18]).reset_index().drop(columns=['index']).set_index(['t', 'j', 'm']).sort_values(['t','j'])\n",
    "Q1 = Q1.drop(columns=['Total Expenditures', 'People per HH', 'Expenditures per capita']) \n",
    "Q1\n",
    "#epc is expenditure per capita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q4_10 = quartiles_by_epc(expend, 2010, 4)\n",
    "Q4_12 = quartiles_by_epc(expend, 2012, 4)\n",
    "Q4_15 = quartiles_by_epc(expend, 2015, 4)\n",
    "Q4_18 = quartiles_by_epc(expend, 2018, 4)\n",
    "Q4 = pd.concat([Q4_10, Q4_12, Q4_15, Q4_18]).reset_index().drop(columns=['index']).set_index(['t', 'j', 'm']).sort_values(['t','j'])\n",
    "Q4 = Q4.drop(columns=['Total Expenditures', 'People per HH', 'Expenditures per capita']) \n",
    "Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Household Dataframe to create one only including 1st quartile households and another including just 4th quartile households."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First Quartile\n",
    "hh_char = hh_char.reorder_levels(['t','j','m'])\n",
    "Q1Index = Q1.index.tolist()\n",
    "Q4Index = Q4.index.tolist()\n",
    "hh_charQ1 = hh_char[hh_char.index.isin(Q1Index)]\n",
    "hh_charQ4 = hh_char[hh_char.index.isin(Q4Index)]\n",
    "hh_charQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fourth Quartile\n",
    "hh_charQ4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logged Food Expenditure Dataframe (after running np.log on values)\n",
    "\n",
    "Q1 = Q1.replace(0,np.nan) \n",
    "Q4 = Q4.replace(0,np.nan) \n",
    "\n",
    "log_Q1 = np.log(Q1)\n",
    "log_Q4 = np.log(Q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Household Size and add to household dataframe for Q1 and Q4\n",
    "\n",
    "# set index to j, t, m so that df.sum() ignore index values\n",
    "hh_charQ1 = hh_charQ1.reset_index()\n",
    "hh_charQ1.set_index(['j','t','m'], inplace=True)\n",
    "hh_charQ4 = hh_charQ4.reset_index()\n",
    "hh_charQ4.set_index(['j','t','m'], inplace=True)\n",
    "\n",
    "# create new column of household size\n",
    "hh_charQ1['Hsize'] = hh_charQ1.sum(axis=1).values\n",
    "hh_charQ4['Hsize'] = hh_charQ4.sum(axis=1).values\n",
    "\n",
    "# remove erroneous data with household_size = 0\n",
    "hh_charQ1 = hh_charQ1[hh_charQ1['Hsize'] > 0]\n",
    "hh_charQ4 = hh_charQ4[hh_charQ4['Hsize'] > 0]\n",
    "\n",
    "# create new column 'log Hsize'\n",
    "hh_charQ1['log Hsize'] = np.log(hh_charQ1['Hsize'])\n",
    "hh_charQ4['log Hsize'] = np.log(hh_charQ4['Hsize'])\n",
    "\n",
    "# remove Hsize column\n",
    "hh_charQ1 = hh_charQ1.drop(columns=['Hsize']) \n",
    "hh_charQ4 = hh_charQ4.drop(columns=['Hsize']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "hh_charQ1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With nothing more than this, we can estimate the demand system.  This\n",
    "happens in two steps.  The first is the &ldquo;reduced form&rdquo; step:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cfe\n",
    "\n",
    "result1 = cfe.Result(y=log_Q1,z=hh_charQ1, min_xproducts = 10)\n",
    "#result4 = cfe.Result(y=log_Q4,z=hh_charQ4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a complicated &ldquo;Result&rdquo; object, with lots of different\n",
    "attributes.  Note from below that attributes $y$ and $z$ are now defined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step of Estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that there are two steps to estimation; the first step\n",
    "involves estimating the &ldquo;reduced form&rdquo; linear regression \n",
    "$$\n",
    "y_{it}^j = {a}_{it} + \\delta_i'{z}^j_t + \\epsilon_{it}^j.\n",
    "$$\n",
    "\n",
    "The Result class has code to estimate this in one line:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.get_reduced_form()\n",
    "#result4.get_reduced_form()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this we can examine the estimated coefficients $\\delta$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result1.delta.to_dataframe().unstack('k')\n",
    "result4.delta.to_dataframe().unstack('k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the good-time constants $a_{it}$ (this captures the effects of prices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.a.to_dataframe().unstack('i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step of Estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step involves using Singular Value Decomposition to find\n",
    "the rank one matrix that best approximates the residuals $e_{it}^j$.\n",
    "This can be interpreted as\n",
    "$$\n",
    "    -\\beta_i\\log\\lambda^j_t,\n",
    "$$\n",
    "where the $\\log\\lambda^j_t$ is the log of the marginal utility of\n",
    "expenditures (MUE) for household $j$ at time $t$, and where $\\beta_i$ are\n",
    "the corresponding &ldquo;Frisch elasticities&rdquo; that tell us how much\n",
    "demand changes as the MUE falls.\n",
    "\n",
    "Estimates can also be computed as a one-liner:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.get_beta(as_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That&rsquo;s all there is to estimation!  Note that we didn&rsquo;t estimate\n",
    "demands for all goods&#x2014;lots of goods didn&rsquo;t have enough observations,\n",
    "and were automatically dropped.  (This can be controlled using the\n",
    "`min_proportion_items` and `min_xproducts` attributes when one\n",
    "instantiates the result object.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment of Fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let&rsquo;s see how we did, by comparing total expenditures predicted by the\n",
    "model we&rsquo;ve estimated with actual total expenditures:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "xbar = np.exp(result.y).sum(['m','i']).to_dataframe('xbar').replace(0,np.nan).squeeze()\n",
    "xhat = result.get_predicted_expenditures().sum(['m','i']).to_dataframe('xhat').replace(0,np.nan).squeeze()\n",
    "\n",
    "# Make dataframe of actual & predicted\n",
    "df = pd.DataFrame({'Actual':np.log(xbar),'Predicted':np.log(xhat)})\n",
    "\n",
    "df.plot.scatter(x='Predicted',y='Actual')\n",
    "\n",
    "# Add 45 degree line\n",
    "v = plt.axis()\n",
    "vmin = np.max([v[0],v[2]])\n",
    "vmax = np.max([v[1],v[3]])\n",
    "plt.plot([vmin,vmax],[vmin,vmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_dataset('icrisat.ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nutritional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdc_table = '1ed8FASRCkN9KwTWTvMzKT6UT4jWbSSZQEwZEmXCt8IQ'\n",
    "\n",
    "fdc_codes = read_sheets(fdc_table,sheet=\"Sheet1\")\n",
    "\n",
    "consumption = read_sheets(nigeria_consumption,sheet='Consumption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fooddatacentral as fdc\n",
    "\n",
    "apikey = 'MfcTfizjo11bsqJeJCn9Tb7RdKPQxJRjJSvTKElr'\n",
    "\n",
    "food_nutrients = {}\n",
    "for f in fdc_codes['Food description'].to_list():\n",
    "    fdc_id = fdc_codes[fdc_codes['Food description'] == f]['USDA FDC ID'].values[0]\n",
    "    if not np.isnan(fdc_id):\n",
    "        try:\n",
    "            fdc_id = int(fdc_id)\n",
    "            food_nutrients[f] = fdc.nutrients(apikey, fdc_id).Quantity\n",
    "        except AttributeError:\n",
    "            warnings.warn(\"Couldn't find FDC Code %s for food %s.\" % (f, fdc_id))\n",
    "\n",
    "nutritional_df = pd.DataFrame(food_nutrients,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dri_mins_sheet = '1XJRHTnxNJwrUXperIhwrwDp1HcVxPEVoQobYDmjg9Qw'\n",
    "\n",
    "dri_mins = read_sheets(dri_mins_sheet,sheet='diet_minimums')\n",
    "dri_mins = dri_mins.reset_index(drop=True).set_index('Nutrition').drop('Source', axis=1)\n",
    "dri_mins"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
